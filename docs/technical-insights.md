# Technical Insights: vLLM Configuration, Performance, and Troubleshooting

## Overview
This document contains comprehensive technical insights extracted from production deployment and testing of Qwen3-480B model with vLLM, including configuration optimization, performance benchmarks, and troubleshooting solutions.

## 1. Working vLLM Model Configurations

### Production Configuration (200k Context)
**File**: `scripts/production/start_qwen3.sh`
```bash
#!/bin/bash
source /opt/vllm/bin/activate

# Optimized environment variables for stability
export VLLM_USE_V1=0                    # Force V0 engine for better memory efficiency
export VLLM_API_KEY='qwen3-secret-key'  # Authentication key
export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1  # Enable long context support
export CUDA_VISIBLE_DEVICES=0,1,2,3     # Use all 4 GPUs
export VLLM_TORCH_COMPILE_LEVEL=0       # Disable memory-hungry compile features

vllm serve /models/qwen3 \
    --served-model-name qwen3 \          # Simplified model name for API
    --tensor-parallel-size 4 \           # 4-GPU tensor parallelism
    --max-model-len 200000 \             # 200k token context window
    --host 0.0.0.0 \                     # Accept external connections
    --port 8000 \                        # Standard port
    --gpu-memory-utilization 0.95 \      # High GPU memory usage
    --enforce-eager \                     # Disable graph compilation for stability
    --trust-remote-code                  # Allow custom model code
```

### High-Context Configuration (700k Context)
**File**: `scripts/production/start_700k_final.sh`
```bash
#!/bin/bash
source /opt/vllm/bin/activate
export VLLM_API_KEY='qwen3-secret-key'
export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
export CUDA_VISIBLE_DEVICES=0,1,2,3

vllm serve /models/qwen3 \
    --tensor-parallel-size 2 \           # Lower TP for memory efficiency
    --pipeline-parallel-size 2 \         # Pipeline parallelism for large context
    --max-model-len 700000 \             # Maximum stable context (700k tokens)
    --kv-cache-dtype fp8 \               # FP8 KV cache for memory efficiency
    --host 0.0.0.0 \
    --port 8000 \
    --gpu-memory-utilization 0.98 \      # Maximum GPU memory usage
    --trust-remote-code
```

## 2. Performance Benchmarks and Optimization

### Measured Performance Metrics
- **Response Latency**: 0.87 seconds for simple requests (excellent for 480B model)
- **Token Generation Speed**: ~75 tokens/second sustained
- **Context Window**: Successfully tested up to 700k tokens
- **Memory Usage**: 15.62 GiB KV cache limit at 200k context
- **GPU Utilization**: 95-98% optimal range

### Performance Optimization Techniques

#### Memory Optimization
1. **FP8 KV Cache**: Reduces memory usage by ~50% for large contexts
   ```bash
   --kv-cache-dtype fp8
   ```

2. **Pipeline Parallelism**: For contexts >200k tokens
   ```bash
   --tensor-parallel-size 2 --pipeline-parallel-size 2
   ```

3. **V0 Engine**: More memory efficient than V1
   ```bash
   export VLLM_USE_V1=0
   ```

#### Stability Optimizations
1. **Disable Graph Compilation**: Prevents memory issues
   ```bash
   --enforce-eager
   export VLLM_TORCH_COMPILE_LEVEL=0
   ```

2. **GPU Memory Utilization**: 95% for stability, 98% for maximum capacity
   ```bash
   --gpu-memory-utilization 0.95  # Stable
   --gpu-memory-utilization 0.98  # Maximum
   ```

## 3. API Authentication and Integration Patterns

### Working API Configuration
- **Authentication**: Bearer token in Authorization header
- **API Key**: `qwen3-secret-key` (configurable)
- **Base URL**: `http://localhost:8000/v1` (OpenAI-compatible)
- **Model ID**: `qwen3` (simplified name, not full path)

### API Testing Results
```json
{
  "models_endpoint_status": "âœ… Working",
  "model_id": "qwen3",
  "max_model_len": 200000,
  "response_time": "0.874 seconds",
  "token_generation": "~75 tokens/second"
}
```

### Authentication Issues and Solutions
**Problem**: HTTP 401 Unauthorized errors
**Solution**: 
1. Ensure API key is set in both environment and vLLM arguments
2. Use exact format: `Authorization: Bearer qwen3-secret-key`
3. Model ID should be `qwen3`, not `/models/qwen3`

## 4. CRUSH CLI Configuration Insights

### Working CRUSH Configuration
**File**: `configs/crush-config.json`
```json
{
  "$schema": "https://charm.land/crush.json",
  "providers": {
    "vllm-local": {
      "type": "openai",
      "base_url": "http://localhost:8000/v1",
      "api_key": "qwen3-secret-key",
      "models": [
        {
          "id": "qwen3",
          "name": "Qwen3-480B Local (200k context)",
          "context_window": 200000,
          "default_max_tokens": 8192,
          "cost_per_1m_in": 0,
          "cost_per_1m_out": 0
        }
      ]
    }
  },
  "default_provider": "vllm-local",
  "default_model": "qwen3",
  "options": {
    "debug": false
  }
}
```

### CRUSH Configuration Locations (macOS)
1. `~/.config/crush/config.json` (primary)
2. `~/Library/Application Support/crush/config.json` (alternative)
3. `./.crush/config.json` (project-local)

### Alternative Environment Variable Setup
```bash
export OPENAI_API_KEY='qwen3-secret-key'
export OPENAI_API_BASE='http://localhost:8000/v1'
crush
```

## 5. Common Issues and Solutions

### Issue 1: HTTP 400 Bad Request
**Symptoms**: CRUSH reaches server but gets 400 error
**Root Cause**: Model name mismatch
**Solution**: Use `qwen3` as model ID, not full path `/models/qwen3`

### Issue 2: "No providers configured" Warning
**Symptoms**: CRUSH logs show provider configuration warnings
**Root Cause**: Config file not found in expected locations
**Solutions**:
1. Create config in `~/.config/crush/config.json`
2. Use environment variables as fallback
3. Run `crush configure` for interactive setup

### Issue 3: Memory Issues with Large Context
**Symptoms**: Out of memory errors, model crashes
**Solutions**:
1. Use FP8 KV cache: `--kv-cache-dtype fp8`
2. Enable pipeline parallelism for >200k context
3. Reduce tensor parallel size for large contexts
4. Set `VLLM_ALLOW_LONG_MAX_MODEL_LEN=1`

### Issue 4: Authentication Failures
**Symptoms**: HTTP 401 Unauthorized
**Solutions**:
1. Verify API key in both `VLLM_API_KEY` and `--api-key`
2. Check Authorization header format
3. Test with curl first before using CRUSH

## 6. Deployment Architecture

### Server Infrastructure
- **Hardware**: 4x H200 GPUs (confirmed working)
- **Model**: Qwen3-480B-A35B-Instruct-FP8
- **Location**: `/models/qwen3`
- **Network**: SSH tunnel on localhost:8000

### Client Setup
- **Local Machine**: macOS with CRUSH CLI
- **Connection**: SSH tunnel to remote server
- **API Access**: OpenAI-compatible endpoint
- **Authentication**: Bearer token

## 7. Performance Tuning Guidelines

### For Maximum Context (700k tokens)
```bash
--tensor-parallel-size 2
--pipeline-parallel-size 2
--kv-cache-dtype fp8
--gpu-memory-utilization 0.98
```

### For Maximum Speed (200k tokens)
```bash
--tensor-parallel-size 4
--max-model-len 200000
--gpu-memory-utilization 0.95
--enforce-eager
```

### For Development/Testing
```bash
--max-model-len 32768
--gpu-memory-utilization 0.85
--tensor-parallel-size 2
```

## 8. Monitoring and Debugging

### Key Metrics to Monitor
- **KV Cache Usage**: Should stay under memory limits
- **Token Generation Speed**: Target >50 tokens/second
- **GPU Memory Utilization**: 85-98% range
- **Response Latency**: <2 seconds for most requests

### Debug Commands
```bash
# Test API connectivity
curl -s http://localhost:8000/v1/models -H "Authorization: Bearer qwen3-secret-key"

# Check model response
curl -s http://localhost:8000/v1/chat/completions \
  -H "Authorization: Bearer qwen3-secret-key" \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3", "messages": [{"role": "user", "content": "Hello"}], "max_tokens": 10}'

# Monitor logs
tail -f /root/vllm_server.log
```

## 9. Production Recommendations

### Stability Configuration
- Use V0 engine (`VLLM_USE_V1=0`)
- Enable eager execution (`--enforce-eager`)
- Set conservative GPU memory (95%)
- Use proven model name (`qwen3`)

### Performance Configuration
- Optimize tensor parallelism based on context needs
- Use FP8 KV cache for large contexts
- Monitor memory usage continuously
- Test thoroughly before production deployment

### Security Considerations
- Use strong API keys in production
- Limit network access to trusted clients
- Monitor API usage and logs
- Regular security updates for vLLM

---

*Generated from production deployment analysis - Last updated: July 31, 2025*